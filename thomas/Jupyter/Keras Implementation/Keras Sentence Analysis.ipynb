{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(\"./data/positive_examples.txt\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(\"./data/negative_examples.txt\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_load_data():\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the MR dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    sentences, labels = load_data_and_labels()\n",
    "    sentences_padded = pad_sentences(sentences)\n",
    "    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "    return [x, y, vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_data = data[shuffle_indices]\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from gensim.models import word2vec\n",
    "from os.path import join, exists, split\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentence_matrix, vocabulary_inv,\n",
    "                   num_features=300, min_word_count=1, context=10):\n",
    "    \"\"\"\n",
    "    Trains, saves, loads Word2Vec model\n",
    "    Returns initial weights for embedding layer.\n",
    "   \n",
    "    inputs:\n",
    "    sentence_matrix # int matrix: num_sentences x max_sentence_len\n",
    "    vocabulary_inv  # dict {int: str}\n",
    "    num_features    # Word vector dimensionality                      \n",
    "    min_word_count  # Minimum word count                        \n",
    "    context         # Context window size \n",
    "    \"\"\"\n",
    "    model_dir = 'models'\n",
    "    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n",
    "    model_name = join(model_dir, model_name)\n",
    "    if exists(model_name):\n",
    "        embedding_model = word2vec.Word2Vec.load(model_name)\n",
    "        print('Load existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
    "    else:\n",
    "        # Set values for various parameters\n",
    "        num_workers = 1  # Number of threads to run in parallel\n",
    "        downsampling = 1e-3  # Downsample setting for frequent words\n",
    "\n",
    "        # Initialize and train the model\n",
    "        print('Training Word2Vec model...')\n",
    "        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n",
    "        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                            size=num_features, min_count=min_word_count,\n",
    "                                            window=context, sample=downsampling)\n",
    "\n",
    "        # If we don't plan to train the model any further, calling \n",
    "        # init_sims will make the model much more memory-efficient.\n",
    "        embedding_model.init_sims(replace=True)\n",
    "        \n",
    "        print('Modelling finished.')\n",
    "\n",
    "        # Saving the model for later use. You can load it later using Word2Vec.load()\n",
    "        if not exists(model_dir):\n",
    "            os.mkdir(model_dir)\n",
    "        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n",
    "        embedding_model.save(model_name)\n",
    "\n",
    "    # add unknown words\n",
    "    embedding_weights = {key: embedding_model[word] if word in embedding_model else\n",
    "                              np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n",
    "                         for key, word in vocabulary_inv.items()}\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Load existing Word2Vec model '300features_1minwords_10context'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:47: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:47: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading data...\")\n",
    "x, _, _, vocabulary_inv_list = helper_load_data()\n",
    "vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
    "w = train_word2vec(x, vocabulary_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Deep Neural Network ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Parameters section -------------------\n",
    "#\n",
    "# Model type. See Kim Yoon's Convolutional Neural Networks for Sentence Classification, Section 3\n",
    "model_type = \"CNN-non-static\"  # CNN-rand|CNN-non-static|CNN-static\n",
    "\n",
    "# Data source\n",
    "data_source = \"keras_data_set\"  # keras_data_set|local_dir\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 300 # original: 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    assert data_source in [\"keras_data_set\", \"local_dir\"], \"Unknown data source\"\n",
    "    x, y, vocabulary, vocabulary_inv_list = helper_load_data()\n",
    "    vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
    "    y = y.argmax(axis=1)\n",
    "\n",
    "    # Shuffle data\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x = x[shuffle_indices]\n",
    "    y = y[shuffle_indices]\n",
    "    train_len = int(len(x) * 0.9)\n",
    "    x_train = x[:train_len]\n",
    "    y_train = y[:train_len]\n",
    "    x_test = x[train_len:]\n",
    "    y_test = y[train_len:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, vocabulary_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Adjusting sequence length for actual size\n",
      "x_train shape: (22500, 2633)\n",
      "x_test shape: (2500, 2633)\n",
      "Vocabulary Size: 81322\n"
     ]
    }
   ],
   "source": [
    "print(\"Load data...\")\n",
    "x_train, y_train, x_test, y_test, vocabulary_inv = load_data()\n",
    "\n",
    "if sequence_length != x_test.shape[1]:\n",
    "    print(\"Adjusting sequence length for actual size\")\n",
    "    sequence_length = x_test.shape[1]\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type is CNN-non-static\n",
      "Load existing Word2Vec model '300features_1minwords_10context'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:47: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:47: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# Prepare embedding layer weights and convert inputs for static model\n",
    "print(\"Model type is\", model_type)\n",
    "if model_type in [\"CNN-non-static\", \"CNN-static\"]:\n",
    "    embedding_weights = train_word2vec(np.vstack((x_train, x_test)), vocabulary_inv, num_features=embedding_dim, min_word_count=min_word_count, context=context)\n",
    "    if model_type == \"CNN-static\":\n",
    "        x_train = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_train])\n",
    "        x_test = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_test])\n",
    "        print(\"x_train static shape:\", x_train.shape)\n",
    "        print(\"x_test static shape:\", x_test.shape)\n",
    "\n",
    "elif model_type == \"CNN-rand\":\n",
    "    embedding_weights = None\n",
    "else:\n",
    "    raise ValueError(\"Unknown model type\")\n",
    "\n",
    "# Build model\n",
    "if model_type == \"CNN-static\":\n",
    "    input_shape = (sequence_length, embedding_dim)\n",
    "else:\n",
    "    input_shape = (sequence_length,)\n",
    "\n",
    "model_input = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding layer with word2vec weights, shape (81322, 300)\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/20\n",
      " - 3242s - loss: 0.6151 - acc: 0.6328 - val_loss: 0.3667 - val_acc: 0.8492\n",
      "Epoch 2/20\n"
     ]
    }
   ],
   "source": [
    "# Static model does not have embedding layer\n",
    "if model_type == \"CNN-static\":\n",
    "    z = model_input\n",
    "else:\n",
    "    z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n",
    "\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "if model_type == \"CNN-non-static\":\n",
    "    weights = np.array([v for v in embedding_weights.values()])\n",
    "    print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "    embedding_layer = model.get_layer(\"embedding\")\n",
    "    embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
